# Spark 连接 PostgreSQL数据库

为了能够访问数据库，你需要在spark classpath中包含特定数据库的JDBC驱动。
想要连接postgres可以在启动命令中添加jars：

```
Scala：
./bin/spark-shell --driver-class-path postgresql-9.4.1207.jar --jars postgresql-9.4.1207.jar

Python：
./bin/pyspark --driver-class-path postgresql-9.4.1207.jar --jars postgresql-9.4.1207.jar

```

首先需要在你的环境中查找，是否存在可用的jdbc jar包。
```
locate jdbc |grep postgres
```
如果没有，需要到网站上下载一个
```
比如，https://jdbc.postgresql.org/download/postgresql-9.4-1205.jdbc4.jar
```

将文件postgresql-9.4-1205.jdbc4.jar放到spark的根目录。

我使用的语言是Python，那么执行下面的命令
```
./bin/pyspark --driver-class-path postgresql-9.4-1205.jdbc4.jar --jars postgresql-9.4-1205.jdbc4.jar

>>> jdbcDF = spark.read \
...     .format("jdbc") \
...     .option("url", "jdbc:postgresql://10.161.30.210:xxxx/postgres") \
...     .option("dbtable", "public.tab") \
...     .option("user", "postgres") \
...     .option("password", "xxxxx") \
...     .load()
>>> jdbcDF.show()
+---+
| id|
+---+
| 10|
| 20|
| 30|
+---+
```

我们连接到了10.161.30.210这个IP地址，端口为xxxx，对应的数据库为postgres。

我在public模式下建了一个表叫做tab，里面只有三条数据。

这时候我们就通过jdbc的方式获取了postgresql数据库的数据，并转换成了dataframe，可以进行后续的操作了。
